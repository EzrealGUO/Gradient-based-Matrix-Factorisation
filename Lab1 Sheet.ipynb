{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d1daa0",
   "metadata": {},
   "source": [
    "# Part 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c596aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this code block to install dependencies when running on colab\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    from os.path import exists\n",
    "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b67fe",
   "metadata": {},
   "source": [
    "## Getting started \n",
    "\n",
    "At its heart, PyTorch is just a library for manipulating tensors. We're going to start learning how to use \n",
    "PyTorch by looking at how we can implement simple linear regression. \n",
    "\n",
    "Code speaks better than words, so lets start by looking at a bit of pytorch code to generate some 2d data to regress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1727e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Generate some data points on a straight line perturbed with Gaussian noise\n",
    "N = 1000 # number of points\n",
    "theta_true = torch.Tensor([[1.5], [2.0]]) # true parameters of the line\n",
    "\n",
    "X = torch.rand(N, 2) \n",
    "X[:, 1] = 1.0\n",
    "y = X @ theta_true + 0.1 * torch.randn(N, 1) \n",
    "# Note that just like in numpy '@' represents matrix multiplication and A@B is equivalent to torch.mm(A, B) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c78198",
   "metadata": {},
   "source": [
    "The above code generates $(x,y)$ data according to $y = 1.5x + 2$, with the $x$'s chosen from a uniform distribution. The $y$'s are additionally purturbed by adding an amount $0.1z$, where $z\\sim \\mathcal{N}(0,1)$ is a sample from a standard normal distribution. \n",
    "\n",
    "Note that we represent our $x$'s as a two-dimensional (row) vector with a 1 in the second element so that the offset can be rolled into the matrix multiplication for efficiency:\n",
    "\n",
    "\\begin{align}\n",
    "    y &= \\mathbf{X}\\begin{bmatrix}\n",
    "           1.5 \\\\\n",
    "           2\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}\n",
    "\n",
    "Let's use `matplotlib` to draw a scatter so we can be sure of what our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4f2d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:,0].numpy(), y.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c6061",
   "metadata": {},
   "source": [
    "__Make sure you understand how the code above is generating data; feel free to change the parameters to see what effect they have.__\n",
    "\n",
    "Now, lets consider the situation where we have been given the tensors $X$ and $y$ and wish to compute the regression parameters. Our model looks like $\\mathbf{y} = \\mathbf{X\\theta}$, and we wish to recover the parameters $\\theta$. \n",
    "\n",
    "As the problem is both overcomplete (only two data pairs are required to find $\\theta$), and the data is noisy, we can use the Moore-Penrose Pseudoinverse to find the least-squares solution to $\\theta$: $\\theta = \\mathbf{X^+y}$. PyTorch has a built-in pseudoinverse method (`pinverse`) that can do all the work for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24518ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4986],\n",
      "        [2.0018]])\n"
     ]
    }
   ],
   "source": [
    "# direct solution using moore-penrose pseudo inverse\n",
    "X_inv = torch.pinverse(X)\n",
    "theta_pinv = torch.mm(X_inv, y)\n",
    "print(theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bfae54",
   "metadata": {},
   "source": [
    "Running the above code should give you a solution vector for $\\theta$ that is very similar to the true parameter vector (`theta_true`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f7a98",
   "metadata": {},
   "source": [
    "## Exercise: computing the pseudoinverse from the Singular Value Decomposition.\n",
    "\n",
    "The standard way of computing the pseudoinverse is by using the Singular Value Decomposition (SVD). The SVD is defined as: $\\mathbf{X} = \\mathbf{U}\\Sigma\\mathbf{V}^\\top$. The pseudoinverse is thus $\\mathbf{X}^+ = \\mathbf{V}\\Sigma^{-1}\\mathbf{U}^\\top$ where $\\Sigma^{-1}$ is a diagonal matrix in which the reciprocal of the corresponding non-zero elements in $\\Sigma$ has been taken.\n",
    "\n",
    "__Use the code block below to compute the parameter vector using the SVD directly rather than the through the `pinverse` method.__ You need to store your manually computed pseudoinverse in `X_inv_svd`. Useful methods will be `torch.svd()` to compute the SVD, `[Tensor].t()` to transpose a matrix and `torch.diag()` to form a diagonal matrix from a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdaad617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7457, 1.0000],\n",
      "        [0.0638, 1.0000],\n",
      "        [0.1882, 1.0000],\n",
      "        ...,\n",
      "        [0.8005, 1.0000],\n",
      "        [0.5988, 1.0000],\n",
      "        [0.0663, 1.0000]])\n",
      "tensor([[-0.0347,  0.0230],\n",
      "        [-0.0256, -0.0509],\n",
      "        [-0.0273, -0.0374],\n",
      "        ...,\n",
      "        [-0.0354,  0.0289],\n",
      "        [-0.0327,  0.0071],\n",
      "        [-0.0257, -0.0506]])\n",
      "tensor([35.5793,  8.1489])\n",
      "tensor([[-0.4708,  0.8822],\n",
      "        [-0.8822, -0.4708]])\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([1000, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 2])\n",
      "____________________________________\n",
      "tensor([[-0.4708, -0.8822],\n",
      "        [ 0.8822, -0.4708]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[35.5793,  0.0000],\n",
      "        [ 0.0000,  8.1489]])\n",
      "tensor([[0.0281, 0.0000],\n",
      "        [0.0000, 0.1227]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[-0.0347, -0.0256, -0.0273,  ..., -0.0354, -0.0327, -0.0257],\n",
      "        [ 0.0230, -0.0509, -0.0374,  ...,  0.0289,  0.0071, -0.0506]])\n",
      "torch.Size([2, 1000])\n",
      "tensor([[-0.0020,  0.0058,  0.0044,  ..., -0.0027, -0.0003,  0.0058],\n",
      "        [-0.0022,  0.0023,  0.0015,  ..., -0.0025, -0.0012,  0.0023]])\n",
      "tensor([[ 0.8287],\n",
      "        [-2.3593]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "uu,ss,vv = torch.svd(X)\n",
    "print(X)\n",
    "print(uu)\n",
    "print(ss)\n",
    "print(vv)\n",
    "print(X.size())\n",
    "print(uu.size())\n",
    "print(ss.size())\n",
    "print(vv.size())\n",
    "print('____________________________________')\n",
    "VV = vv.t()\n",
    "print(VV)\n",
    "print(VV.size())\n",
    "SS = torch.diag(ss)\n",
    "print(SS)\n",
    "SS= torch.pinverse(SS)\n",
    "print(SS)\n",
    "print(SS.size())\n",
    "UU = uu.t()\n",
    "print(UU)\n",
    "print(UU.size())\n",
    "X_inv_svd = torch.mm(torch.mm(VV,SS),UU)\n",
    "print(X_inv_svd)\n",
    "theta_pinv_svd = torch.mm(X_inv_svd, y)\n",
    "print(theta_pinv_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d542dc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(torch.lt(torch.abs(torch.add(theta_pinv, -theta_pinv_svd)), 1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db1571",
   "metadata": {},
   "source": [
    "## Gradient based Linear Regression\n",
    "\n",
    "Fundamentally, with linear regression we are trying to find a solution vector, $theta$ that minimises $f(\\theta) = 0.5\\|\\mathbf{X}\\theta - \\mathbf{y}\\|_2^2$. \n",
    "\n",
    "We've already seen how this can be minimised directly using the pseudoinverse, but it could also be minimised by using gradient descent: $\\theta \\gets \\theta - \\alpha f'(\\theta)$. (_Interesting aside_: SVD (and thus the pseudoinverse) can also be solved using gradient methods - in fact this becomes the only practical way for really large matrices.).\n",
    "\n",
    "__Use the following block to derive and write down the gradient, $f'(\\theta)$, of $f(\\theta)$__. Note that you can insert latex code by wrapping expressions in dollar symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c8b53",
   "metadata": {},
   "source": [
    "__Now complete the following code block to implement your gradient as pytorch code:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ca5e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear_regression_loss_grad(theta, X, y):\n",
    "    X0 = X.t()\n",
    "    grad  = torch.mm(X0,(torch.mm(X,theta)-y))\n",
    "    # theta, X and y have the same shape as used previously\n",
    "    return grad\n",
    "\n",
    "linear_regression_loss_grad(torch.zeros(2,1), X, y).shape == (2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059b731",
   "metadata": {},
   "source": [
    "Now we can plug that gradient function into a basic gradient descent solver and check that the solution is close to what we get with the pseudoinverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa6a0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4986],\n",
      "        [2.0018]])\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "theta = torch.Tensor([[0], [0]])\n",
    "for e in range(0, 200):\n",
    "    gr = linear_regression_loss_grad(theta, X, y)\n",
    "    theta -= alpha * gr\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978cc58",
   "metadata": {},
   "source": [
    "## Real data\n",
    "\n",
    "Doing linear regression on synthetic data is a great way to understand how PyTorch works, but it isn't quite as satisfying as working with a real dataset. Let's now apply or understanding of computing linear regression parameters to a dataset of house prices in Boston.\n",
    "\n",
    "We'll load the dataset using scikit-learn and perform some manipulations in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905f1823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([506, 3])\n",
      "y: torch.Size([506, 1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "X = torch.Tensor(data) #convert to pytorch Tensors\n",
    "y = torch.Tensor(target)\n",
    "\n",
    "X = X[:, [2,5]] # We're just going to use features 2 and 5, rather than using all of of them\n",
    "X = torch.cat((X, torch.ones((X.shape[0], 1))), 1) # append a column of 1's to the X's\n",
    "y = y.reshape(-1, 1) # reshape y into a column vector\n",
    "print('X:', X.shape)\n",
    "print('y:', y.shape)\n",
    "\n",
    "# We're also going to break the data into a training set for computing the regression parameters\n",
    "# and a test set to evaluate the predictive ability of those parameters\n",
    "perm = torch.randperm(y.shape[0])\n",
    "X_train = X[perm[0:253], :]\n",
    "y_train = y[perm[0:253]]\n",
    "X_test = X[perm[253:], :]\n",
    "y_test = y[perm[253:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe4ff42",
   "metadata": {},
   "source": [
    "__Use the following code block to compute the regression parameters using the training data in the variable `theta` by solving using the pseudoinverse directly:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71de2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the regression parameters in variable theta\n",
    "X_train_inv = torch.pinverse(X_train)\n",
    "theta = torch.mm(X_train_inv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583cb4e",
   "metadata": {},
   "source": [
    "We can now print out the error achieved on the test set, as well as the parameter vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3069dcec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape == (3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6071093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta:  tensor([[ -0.3213,   6.2526, -13.1900]])\n",
      "MSE of test data:  tensor(35.1216)\n"
     ]
    }
   ],
   "source": [
    "print(\"Theta: \", theta.t())\n",
    "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8fc99",
   "metadata": {},
   "source": [
    "Now let's try using gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f2ddedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Theta:  tensor([[-0.4118,  4.5354, -1.3322]])\n",
      "MSE of test data:  tensor(40.6365)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.00001\n",
    "theta_gd = torch.rand((X_train.shape[1], 1))\n",
    "for e in range(0, 10000):\n",
    "    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "print(\"Gradient Descent Theta: \", theta_gd.t())\n",
    "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d099f5",
   "metadata": {},
   "source": [
    "__Use the following block to note down any observations you can make about the choice of learning rate and number of iterations in the above code. What factors do you think influence the choice?__\n",
    "\n",
    "*Alpha = 0.00001 is the best; bigger get stuck, smaller hard converging\n",
    "Iteration bigger, slower but better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "857dc65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Theta:  tensor([[nan, nan, nan]])\n",
      "MSE of test data:  tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "theta_gd = torch.rand((X_train.shape[1], 1))\n",
    "for e in range(0, 10000):\n",
    "    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "print(\"Gradient Descent Theta: \", theta_gd.t())\n",
    "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e574baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Theta:  tensor([[nan, nan, nan]])\n",
      "MSE of test data:  tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.0001\n",
    "theta_gd = torch.rand((X_train.shape[1], 1))\n",
    "for e in range(0, 10000):\n",
    "    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "print(\"Gradient Descent Theta: \", theta_gd.t())\n",
    "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c02d5463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Theta:  tensor([[-0.4039,  4.0540,  1.4259]])\n",
      "MSE of test data:  tensor(46.9519)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.0000001\n",
    "theta_gd = torch.rand((X_train.shape[1], 1))\n",
    "for e in range(0, 10000):\n",
    "    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "print(\"Gradient Descent Theta: \", theta_gd.t())\n",
    "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34310fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Theta:  tensor([[nan, nan, nan]])\n",
      "MSE of test data:  tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.00005\n",
    "theta_gd = torch.rand((X_train.shape[1], 1))\n",
    "for e in range(0, 10000):\n",
    "    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "print(\"Gradient Descent Theta: \", theta_gd.t())\n",
    "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59b9e436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Theta:  tensor([[ -0.2778,   8.2298, -25.9742]])\n",
      "MSE of test data:  tensor(42.0210)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.00001\n",
    "theta_gd = torch.rand((X_train.shape[1], 1))\n",
    "for e in range(0, 1000000):\n",
    "    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "print(\"Gradient Descent Theta: \", theta_gd.t())\n",
    "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29bb8253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Theta:  tensor([[-0.4052,  4.1290,  0.9867]])\n",
      "MSE of test data:  tensor(46.6942)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.00001\n",
    "theta_gd = torch.rand((X_train.shape[1], 1))\n",
    "for e in range(0, 100):\n",
    "    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "print(\"Gradient Descent Theta: \", theta_gd.t())\n",
    "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "192c59a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent Theta:  tensor([[ -0.3215,   6.2491, -13.1660]])\n",
      "MSE of test data:  tensor(35.1301)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.00001\n",
    "theta_gd = torch.rand((X_train.shape[1], 1))\n",
    "for e in range(0, 1000000):\n",
    "    gr = linear_regression_loss_grad(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "print(\"Gradient Descent Theta: \", theta_gd.t())\n",
    "print(\"MSE of test data: \", torch.nn.functional.mse_loss(X_test @ theta_gd, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b1cea",
   "metadata": {},
   "source": [
    "Finally, just so we can visualise what our model has learned, we can plot the predicted house prices (from both the direct solution and from gradient descent) along with the true value for each of the houses in the test set (ordered by increasing true value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "669491b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perm = torch.argsort(y_test, dim=0)\n",
    "plt.plot(y_test[perm[:,0]].numpy(), '.', label='True Prices')\n",
    "plt.plot((X_test[perm[:,0]] @ theta).numpy(), '.', label='Predicted (pinv)')\n",
    "plt.plot((X_test[perm[:,0]] @ theta_gd).numpy(), '+', label='Predicted (G.D.)')\n",
    "plt.xlabel('House Number')\n",
    "plt.ylabel('House Price ($,000s)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f0625",
   "metadata": {},
   "source": [
    "# Part 2: Logistic Regression and Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aba06f",
   "metadata": {},
   "source": [
    "In the first part of the lab we saw how to make predictions of continously varying values with a linear regression model. Lets now turn our focus to binary classification using a simple classification algorithm known as Logistic regression.\n",
    "\n",
    "In linear regression we tried to predict the value of $y$ for an example $\\mathbf{x}$ using a linear function $y=\\mathbf{x}^\\top\\theta$ (where $\\mathbf{x}$ and $\\theta$ are column-vectors). This will clearly not be a great solution for predicting binary-valued labels ($y\\in\\{0,1\\}$). In logistic regression we use a different hypothesis class to try to predict the probability that a given example belongs to the \"1\" class versus the probability that it belongs to the \"0\" class. Specifically, we will try to learn a function of the form:\n",
    "\n",
    "\\begin{align}\n",
    "P(y=1|\\mathbf{x}) &= \\frac{1}{1 + \\exp(-\\mathbf{x}^\\top\\theta)} \\equiv \\sigma(\\mathbf{x}^\\top\\theta),\\\\\n",
    "P(y=0|\\mathbf{x}) &= 1 - P(y=1|\\mathbf{x}).\n",
    "\\end{align}\n",
    " \n",
    "The function $\\sigma(z) \\equiv \\frac{1}{1 + \\exp(-z)}$ is called the \"sigmoid\" or \"logistic\" function. The sigmoid function squashes any real valued input into the range $[0,1]$ enabling us to interprete the output as a probability. Our goal is to search for a value of $\\theta$ so that the probability $P(y=1|\\mathbf{x})=\\sigma(\\mathbf{x}^\\top\\theta)$ is large when $\\mathbf{x}$ belongs to the \"1\" class and small when $\\mathbf{x}$ belongs to the \"0\" class (so that $P(y=0|\\mathbf{x})$ is large). \n",
    "\n",
    "With Linear Regression, the natural cost function was one that measured the sum of squared residuals (the difference between the predicted value and true value). With logisitic regression we have a probabilisitic model, so it makes sense that we use a function that measures the likelihood of the data given the model (note that we want to maximise this function rather than minimise it). As an aside, note that in the case of linear regression if we assume that the data has errors that are IID (independently and identically distributed) according to a Normal distribution, then it can be shown that the maximising the likelihood is exactly the same as minimising the sum of squared residuals. For logistic regression, the likelihood function for a single data point is:\n",
    "\n",
    "\\begin{align}\n",
    "p(y|\\mathbf{x}; \\theta) &= \\sigma(\\mathbf{x}^\\top\\theta)^y(1-\\sigma(\\mathbf{x}^\\top\\theta)^{(1-y)}.\n",
    "\\end{align}\n",
    "\n",
    "(Eliminating another term when y = 0 or 1)\n",
    "\n",
    "For a complete dataset of points $(y_i, \\mathbf{x}_i)$, then the complete likelihood is:\n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) &= \\prod_i \\sigma(\\mathbf{x}_i^\\top\\theta)^{y_i}(1-\\sigma(\\mathbf{x}_i^\\top\\theta)^{(1-y_i)}\n",
    "\\end{align}\n",
    "\n",
    "However, it is considerably easier to maximise the log-likelihood function:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{l}(\\theta) &= \\log L(\\theta) \\\\\n",
    "                    & = \\log \\prod_i \\sigma(\\mathbf{x}_i^\\top\\theta)^{y_i}(1-\\sigma(\\mathbf{x}_i^\\top\\theta)^{(1-y_i)} \\\\\n",
    "                    & = \\sum_i y_i \\log(\\sigma(\\mathbf{x}_i^\\top\\theta)) + (1-y_i) \\log(1-\\sigma(\\mathbf{x}_i^\\top\\theta))\n",
    "\\end{align}\n",
    "\n",
    "Clearly, maximising the log-likelihood is equivalent to minimising the negative log-likelihood. The negative of the log-likelihood function having the form $-\\sum_i y_i \\log(p) + (1-y_i) \\log(p)$, where p is a function returning the predicted probability of class \"1\", is often called the __\"Binary Cross Entropy\"__ function, __\"Binary Cross Entropy Loss\"__ or sometimes the __\"log loss\"__.\n",
    "\n",
    "For conciseness and computational efficiency, we can write the negative logistic regression log-likelihood function in matrix form. Assuming the $y_i$ are stored in a column vector $\\mathbf{y}$ and the data vectors $x_i$ in the __rows__ of a matrix $\\mathbf{X}$, then: \n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{NLL}(\\theta) & = -(\\mathbf{y}^\\top \\log(\\sigma(\\mathbf{X}\\theta)) + (1-\\mathbf{y})^\\top \\log(1-\\sigma(\\mathbf{X}\\theta)))\n",
    "\\end{align}\n",
    "\n",
    "The gradients of this function are given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathrm{NLL}(\\theta) & = \\mathbf{X}^\\top(\\sigma(\\mathbf{X}\\theta) - \\mathbf{y})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1064fb",
   "metadata": {},
   "source": [
    "__Use the box below to compute the gradients of the negative log-likelihood function $\\nabla_\\theta \\mathrm{NLL}(\\theta)$. You can use `torch.sigmoid()` to apply the sigmoid function.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee842c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# we wouldn't normally do this, but for this lab we want to work in double precision\n",
    "# as we'll need the numerical accuracy later on for doing checks on our gradients:\n",
    "torch.set_default_dtype(torch.float64) \n",
    "\n",
    "def logistic_regression_loss_grad(theta, X, y):\n",
    "    # YOUR CODE HERE\n",
    "    grad = torch.mm(X.t(),(torch.sigmoid(X @ theta) - y))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41aa49ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = torch.zeros(1)\n",
    "X = torch.Tensor([[1]])\n",
    "y = torch.Tensor([[0]])\n",
    "logistic_regression_loss_grad(theta, X, y) == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264d18f1",
   "metadata": {},
   "source": [
    "## Training a Logistic Regressor with real data\n",
    "\n",
    "We'll now try gradient descent using our gradient function on a real dataset from `scikit-learn` called `digits`. \n",
    "\n",
    "The `digits` dataset contains handwritten characters (much like the `MNIST` dataset that you may have heard of - we'll explore `MNIST` in a future lab). As logistic regression is a binary classifier, we'll just use the first 2 characters (0 and 1) from the `digits` dataset, and make our own training and test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25316c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data, target = load_digits(n_class=2, return_X_y=True)\n",
    "X = torch.Tensor(data)\n",
    "y = torch.Tensor(target) #convert to pytorch Tensors\n",
    "X = torch.cat((X, torch.ones((X.shape[0], 1))), 1) # append a column of 1's to the X's\n",
    "y = y.reshape(-1, 1) # reshape y into a column vector\n",
    "\n",
    "# We're also going to break the data into a training set for computing the regression parameters\n",
    "# and a test set to evaluate the predictive ability of those parameters\n",
    "perm = torch.randperm(y.shape[0])\n",
    "X_train = X[perm[0:260], :]\n",
    "y_train = y[perm[0:260]]\n",
    "X_test = X[perm[260:], :]\n",
    "y_test = y[perm[260:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa63ff8",
   "metadata": {},
   "source": [
    "Now we have the data, we can use our loss function to try and estimate the optimal parameters for the two-digit classification problem. We'll use `PyTorch`s `torch.nn.functional.binary_cross_entropy_with_logits` function to print out the Binary Cross Entropy of the training data at each iteration, and of the test data once the optimisation is complete. \n",
    "\n",
    "Note: `logits` refers to unscaled probabilities before the sigmoid is applied, so in the `binary_cross_entropy_with_logits` function we just pass in $\\mathbf{X}\\theta$. `PyTorch` does also have a `torch.nn.binary_cross_entropy` method that takes in probabilities, however, as we'll see when implementing neural networks in a later lab, we'll often choose to work with logits as they provide better numerical stability thanks to the _log-sum-exp_ trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fc05f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([260, 65])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f75a4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  BCE of training data: tensor(70.6604)\n",
      "Epoch: 1  BCE of training data: tensor(40.9157)\n",
      "Epoch: 2  BCE of training data: tensor(10.6069)\n",
      "Epoch: 3  BCE of training data: tensor(0.0103)\n",
      "Epoch: 4  BCE of training data: tensor(0.0022)\n",
      "Epoch: 5  BCE of training data: tensor(0.0007)\n",
      "Epoch: 6  BCE of training data: tensor(0.0004)\n",
      "Epoch: 7  BCE of training data: tensor(0.0003)\n",
      "Epoch: 8  BCE of training data: tensor(0.0002)\n",
      "Epoch: 9  BCE of training data: tensor(0.0002)\n",
      "Gradient Descent Theta: tensor([[ 0.8178,  0.8849,  0.0442, -0.7175, -0.1383,  0.9468,  0.5034,  0.3890,\n",
      "          0.7860,  0.4214, -1.3169, -0.4351,  0.0959, -0.0233,  0.9525,  0.8439,\n",
      "          0.4454,  0.1152, -1.7166,  1.5928,  2.2634, -1.1212, -0.4840,  0.5561,\n",
      "          0.5700, -0.3865, -0.4834,  2.9374,  2.8723, -0.5896, -0.9949,  0.6594,\n",
      "          0.2673, -0.8247, -1.0694,  2.1128,  3.3307, -0.3576, -0.8857,  0.3952,\n",
      "          0.3675, -0.3407, -1.3731,  2.4717,  3.0764, -0.7557, -0.7338,  0.1468,\n",
      "          0.0074, -0.0255, -2.0130,  0.0352,  0.4573, -1.3613,  0.5467,  0.8400,\n",
      "          0.7493,  0.4311, -0.1557, -1.2196, -0.4641,  0.8509,  0.8106,  1.1021,\n",
      "          0.0718]])\n",
      "BCE of test data: tensor(2.3632e-11)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "theta_gd = torch.rand((X_train.shape[1], 1))\n",
    "for e in range(0, 10):\n",
    "    gr = logistic_regression_loss_grad(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "    print(\"Epoch:\", e, \" BCE of training data:\", torch.nn.functional.binary_cross_entropy_with_logits(X_train @ theta_gd, y_train))\n",
    "\n",
    "print(\"Gradient Descent Theta:\", theta_gd.t())\n",
    "print(\"BCE of test data:\", torch.nn.functional.binary_cross_entropy_with_logits(X_test @ theta_gd, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3621fdb",
   "metadata": {},
   "source": [
    "What do you observe from running the above? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a10cd7",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "How can we be sure that our gradient function was correct? We might have made an error in the analytical derivation or in the implementation of that derivation into code. Even though we observed the optimisation process on real data converge (hopefully!), you might have made a subtle error in the implementation...\n",
    "\n",
    "So far we have worked with relatively simple algorithms where it is straightforward to compute the objective function and its gradient with pen-and-paper, and then implement the necessary computations in PyTorch. For more complex models that we will see later, the gradient computation can be notoriously difficult to debug and get right. Sometimes a subtly buggy implementation will manage to learn something that can look surprisingly reasonable (while performing less well than a correct implementation). Thus, even with a buggy implementation, it may not at all be apparent that anything is amiss. In this section, we describe a method for numerically checking the derivatives computed by your code to make sure that your implementation is correct. Carrying out the derivative checking procedure described here will significantly increase your confidence in the correctness of your code.\n",
    "\n",
    "Suppose we want to minimize $J(\\theta)$ as a function of $\\theta$. For this example, suppose $J:\\mathbb{R}\\mapsto\\mathbb{R}$, so that $\\theta \\in \\mathbb{R}$. If we are using gradient descent (or other gradient-based optimisation function), then we usually have implemented some function $g(\\theta)$ that purportedly computes $\\frac{d}{d\\theta}J(\\theta)$.\n",
    "\n",
    "How can we check if our implementation of $g$ is correct?\n",
    "\n",
    "Recall the mathematical definition of the derivative is:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d}{d\\theta}J(\\theta) = \\lim_{\\epsilon \\rightarrow 0}\n",
    "\\frac{J(\\theta+ \\epsilon) - J(\\theta-\\epsilon)}{2 \\epsilon}.\n",
    "\\end{align}\n",
    "\n",
    "Thus, at any specific value of $\\theta$, we can numerically approximate the derivative as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{J(\\theta+{\\rm EPSILON}) - J(\\theta-{\\rm EPSILON})}{2 \\times {\\rm EPSILON}}\n",
    "\\end{align}\n",
    " \n",
    "In practice, we set ${\\rm EPSILON}$ to a small constant, say around $10^{−4}$. (There is a large range of values of EPSILON values that should work well, but we don’t set ${\\rm EPSILON}$ to be \"extremely\" small, say $10^{−20}$, as that would lead to numerical roundoff errors.)\n",
    "\n",
    "Thus, given a function $g(\\theta)$ that is supposedly computing $\\frac{d}{d\\theta}J(\\theta)$, we can now numerically verify its correctness by checking that\n",
    "\n",
    "\\begin{align}\n",
    "g(\\theta) \\approx\n",
    "\\frac{J(\\theta+{\\rm EPSILON}) - J(\\theta-{\\rm EPSILON})}{2 \\times {\\rm EPSILON}}.\n",
    "\\end{align}\n",
    " \n",
    "The degree to which these two values should approximate each other will depend on the details of $J$. But assuming ${\\rm EPSILON}=10^{−4}$, you’ll usually find that the left- and right-hand sides of the above will agree to at least 4 significant digits (and often many more).\n",
    "\n",
    "Now, consider the case where $\\theta \\in \\mathbb{R}^n$ is a vector rather than a single real number (so that we have $n$ parameters that we want to learn), and $J: \\mathbb{R}^n \\mapsto \\mathbb{R}$. We now generalize our derivative checking procedure to the case where $\\theta$ may be a vector (as in our linear regression and logistic regression examples).\n",
    "\n",
    "Suppose we have a function $g_i(\\theta)$ that purportedly computes $\\frac{\\partial}{\\partial\\theta_i}J(\\theta)$; we’d like to check if $g_i$ is outputting correct derivative values. Let $\\textstyle \\theta^{(i+)} = \\theta + {\\rm EPSILON} \\times \\vec{e}_i$, where\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{e}_i = \\begin{bmatrix}0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "is the $i$-th basis vector (a vector of the same dimension as $\\theta$, with a \"1\" in the $i$-th position and \"0\"s everywhere else). So, $\\theta^{(i+)}$ is the same as $\\theta$, except its $i$-th element has been incremented by ${\\rm EPSILON}$. Similarly, let $\\theta^{(i−)}=\\theta−{\\rm EPSILON} \\times \\vec{e}_i$ be the corresponding vector with the $i$-th element decreased by ${\\rm EPSILON}$.\n",
    "\n",
    "We can now numerically verify $g_i(\\theta)$'s correctness by checking, for each $i$, that:\n",
    "\n",
    "\\begin{align}\n",
    "g_i(\\theta) \\approx\n",
    "\\frac{J(\\theta^{(i+)}) - J(\\theta^{(i-)})}{2 \\times {\\rm EPSILON}}.\n",
    "\\end{align}\n",
    "\n",
    "### Gradient checker code\n",
    "\n",
    "The following code block contains an implementation of the gradient checking proceedure described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e064950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "def grad_check(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
    "    sum_error = 0\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape]) #randomly sample value to change\n",
    "\n",
    "        oldval = x[ix].item()\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evaluate f(x + h)\n",
    "        x[ix] = oldval - h # increment by h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic) + 1e-8)\n",
    "        sum_error += rel_error\n",
    "        print('numerical: %f\\tanalytic: %f\\trelative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
    "    return sum_error / num_checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c609b",
   "metadata": {},
   "source": [
    "To use the gradient checker, we provide our analytical gradients, together with a function that computes the actual loss (rather than the gradients of the loss) and the parameters at which the gradient was computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "425ad582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -231.144800\tanalytic: -231.144800\trelative error: 7.637085e-11\n",
      "numerical: 96.512932\tanalytic: 96.512932\trelative error: 6.578376e-12\n",
      "numerical: 4.632942\tanalytic: 4.632942\trelative error: 7.851553e-12\n",
      "numerical: 0.528250\tanalytic: 0.528250\trelative error: 9.163149e-11\n",
      "numerical: 96.512932\tanalytic: 96.512932\trelative error: 6.578376e-12\n",
      "numerical: 179.022446\tanalytic: 179.022446\trelative error: 1.451443e-10\n",
      "numerical: 87.635239\tanalytic: 87.635239\trelative error: 4.745448e-10\n",
      "numerical: 0.000000\tanalytic: 0.000000\trelative error: 0.000000e+00\n",
      "numerical: 0.169199\tanalytic: 0.169199\trelative error: 2.360763e-09\n",
      "numerical: 120.254724\tanalytic: 120.254724\trelative error: 2.385832e-10\n",
      "average error: tensor([[3.4080e-10]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we'll use random parameters:\n",
    "theta = torch.rand_like(theta_gd)*0.001\n",
    "# and compute the analytic gradient (w.r.t the test data we loaded in this case)\n",
    "grad = logistic_regression_loss_grad(theta, X_test, y_test)\n",
    "\n",
    "# we need a function that computes the loss for a given theta (and implicitly the data)\n",
    "def func(th):\n",
    "    sigm = torch.sigmoid(X_test @ th)\n",
    "    f = -(y_test.t() @ torch.log(sigm) + (1 - y_test.t()) @ torch.log(1 - sigm));\n",
    "    return f\n",
    "\n",
    "# and run the gradient checker\n",
    "relerr = grad_check(func, theta, grad)\n",
    "print(\"average error:\", relerr)\n",
    "\n",
    "relerr < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0322f",
   "metadata": {},
   "source": [
    "Running the above, you should have a very small average error, and the relative error for each trial should also be a very small value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61acb5b6",
   "metadata": {},
   "source": [
    "# Part 3: Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f6824",
   "metadata": {},
   "source": [
    "In the second part of the lab we saw how to make a linear binary classifier using logisitic regression. In this  part of the lab we'll turn our attention to multi-class classification.\n",
    "\n",
    "Softmax regression (or multinomial logistic regression) is a generalisation of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: $y_i\\in \\{0,1\\}$. We used such a classifier to distinguish between two kinds of hand-written digits. Softmax regression allows us to handle $y_i \\in \\{1,\\dots,K\\}$ where $K$ is the number of classes.\n",
    "\n",
    "Recall that in logistic regression, we had a training set $\\{(\\mathbf{x}_1,y_1),\\dots,(\\mathbf{x}_m,y_m)\\}$ of $m$ labeled examples, where the input features are $\\mathbf{x}_i \\in \\mathbb{R}^n$. In logistic regression, our hypothesis took the form:\n",
    "\n",
    "\\begin{align}\n",
    "h_\\theta(\\mathbf{x}) &= \\frac{1}{1 + \\exp(-\\mathbf{x}^\\top\\theta)} \\equiv \\sigma(\\mathbf{x}^\\top\\theta)\n",
    "\\end{align}\n",
    "\n",
    "and the model parameters $\\theta$ were trained to minimise the cost function\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) & = \\sum_i y_i \\log(\\sigma(\\mathbf{x}_i^\\top\\theta)) + (1-y_i) \\log(1-\\sigma(\\mathbf{x}_i^\\top\\theta))\n",
    "\\end{align}\n",
    "\n",
    "In the softmax regression setting, we are interested in multi-class classification, and so the label $y$\n",
    " can take on $K$ different values, rather than only two. Thus, in our training set $\\{(\\mathbf{x}_1,y_1),\\dots,(\\mathbf{x}_m,y_m)\\}$, we now have that $y_i \\in \\{1,\\dots,K\\}$.\n",
    "\n",
    "Given a test input $\\mathbf{x}$, we want our hypothesis to estimate the probability that $P(y=k|\\mathbf{x})$ for each value of $k=1,\\dots,K$. That is to say, we want to estimate the probability of the class label taking on each of the $K$ different possible values. Thus, our hypothesis will output a $K$-dimensional vector (whose elements sum to 1) giving us our $K$ estimated probabilities. Concretely, our hypothesis $h_\\theta(\\mathbf{x})$ takes the form:\n",
    "\n",
    "\\begin{align}\n",
    "h_\\theta(\\mathbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\theta) \\\\\n",
    "P(y = 2 | \\mathbf{x}; \\theta) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = K | \\mathbf{x}; \\theta)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{j=1}^{K}{\\exp(\\theta^{(j)\\top} \\mathbf{x}) }}\n",
    "\\begin{bmatrix}\n",
    "\\exp(\\theta^{(1)\\top} \\mathbf{x} ) \\\\\n",
    "\\exp(\\theta^{(2)\\top} \\mathbf{x} ) \\\\\n",
    "\\vdots \\\\\n",
    "\\exp(\\theta^{(K)\\top} \\mathbf{x} ) \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Here $\\theta^{(1)},\\theta^{(2)},\\dots,\\theta^{(K)} \\in \\mathbb{R}^n$ are the parameters of our model. Notice that the term $\\frac{1}{\\sum_{j=1}^K exp(\\theta^{(j)\\top} \\mathbf{x})}$ normalizes the distribution, so that it sums to one.\n",
    "\n",
    "For convenience, we will also write $\\theta$ to denote all the parameters of our model. When you implement softmax regression, it is usually convenient to represent $\\theta$ as a $n$-by-$K$ matrix obtained by concatenating $\\theta_{(1)},\\theta^{(2)},\\dots,\\theta^{(K)}$ into columns, so that\n",
    "\n",
    "\\begin{align}\n",
    "\\theta = \\left[\\begin{array}{cccc}| & | & | & | \\\\\n",
    "\\theta^{(1)} & \\theta^{(2)} & \\cdots & \\theta^{(K)} \\\\\n",
    "| & | & | & |\n",
    "\\end{array}\\right].\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fbc67",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "We now describe the cost function that we’ll use for softmax regression. In the equation below, $1\\{\\cdot\\}$\n",
    " is an \"indicator function\", such that $1\\{\\mathrm{a true statement}\\}=1$, and $1\\{\\mathrm{a false statement}\\}=0$. For example, $1\\{2+2=4\\}$ evaluates to $1$; whereas $1\\{1+1=5\\}$ evaluates to $0$. Our cost function will be:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) = - \\left[ \\sum_{i=1}^{m} \\sum_{k=1}^{K}  1\\left\\{y_{i} = k\\right\\} \\log \\frac{\\exp(\\theta^{(k)\\top} \\mathbf{x}_i)}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} \\mathbf{x}_i)}\\right]\n",
    "\\end{align}\n",
    " \n",
    "Notice that this generalises the logistic regression cost function, which could also have been written:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) &= - \\left[ \\sum_{i=1}^m   (1-y^{(i)}) \\log (1-h_\\theta(\\mathbf{x}_i)) + y^{(i)} \\log h_\\theta(\\mathbf{x}_i) \\right] \\\\\n",
    "&= - \\left[ \\sum_{i=1}^{m} \\sum_{k=0}^{1} 1\\left\\{y^{(i)} = k\\right\\} \\log P(y^{(i)} = k | \\mathbf{x}_i ; \\theta) \\right]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The softmax cost function is similar, except that we now sum over the $K$ different possible values of the class label. Note also that in softmax regression, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "P(y_i = k | \\mathbf{x}_i ; \\theta) = \\frac{\\exp(\\theta^{(k)\\top} \\mathbf{x}_i)}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} \\mathbf{x}_i) }\n",
    "\\end{equation}\n",
    "\n",
    "We cannot solve for the minimum of $J(\\theta)$ analytically, and thus we'll resort to using gradient descent as before. Taking derivatives, one can show that the gradient is:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\theta^{(k)}} J(\\theta) = - \\sum_{i=1}^{m}{ \\left[ \\mathbf{x}_i \\left( 1\\{ y_i = k\\}  - P(y_i = k | \\mathbf{x}_i; \\theta) \\right) \\right]  }\n",
    "\\end{align}\n",
    "\n",
    "Armed with this formula for the derivative, one can then use it directly with a gradient descent solver (or any other 1st-order gradient based optimiser)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f55556",
   "metadata": {},
   "source": [
    "__Use the code box below to complete the implementation of the functions that return the gradients of the softmax loss function, $\\nabla_{\\theta^{(k)}} J(\\theta) \\,\\, \\forall k$ and the loss function itself, $J(\\theta)$:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "78b71b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# we wouldn't normally do this, but for this lab we want to work in double precision\n",
    "# as we'll need the numerical accuracy later on for doing checks on our gradients:\n",
    "torch.set_default_dtype(torch.float64) \n",
    "\n",
    "def softmax_regression_loss_grad(Theta, X, y):\n",
    "    '''Implementation of the gradient of the softmax loss function.\n",
    "    \n",
    "    Theta is the matrix of parameters, with the parameters of the k-th class in the k-th column\n",
    "    X contains the data vectors (one vector per row)\n",
    "    y is a column vector of the targets\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    k = Theta.shape[1]\n",
    "    Sum = torch.sum(torch.exp(X@Theta),dim = 1,keepdim = False)\n",
    "    grad = torch.zeros((m,n))\n",
    "    for i in range(k):\n",
    "        for j in range(m):\n",
    "            P = torch.exp(Theta[:,i]@X[j,:])/Sum[j]\n",
    "            l = 1* (y[j] == i)\n",
    "            grad[i] -= X[j,:]*(l-P)\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    return grad\n",
    "\n",
    "def softmax_regression_loss(Theta, X, y):\n",
    "    '''Implementation of the softmax loss function.\n",
    "        \n",
    "    Theta is the matrix of parameters, with the parameters of the k-th class in the k-th column\n",
    "    X contains the data vectors (one vector per row)\n",
    "    y is a column vector of the targets\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    m = X.shape[0]\n",
    "    k = Theta.shape[1]\n",
    "    Sum = torch.sum(torch.exp(X@Theta),dim = 1,keepdim = False)\n",
    "    LossMatrix = torch.zeros((m,1))\n",
    "    for i in range(m):\n",
    "        for j in range(k):\n",
    "            P = torch.exp(Theta[:,j]@X[i,:])/Sum[i]\n",
    "            l = 1* (y[i] == j)\n",
    "            LossMatrix[i] -= l * torch.log(P)\n",
    "    loss = torch.sum(LossMatrix)\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaecc51b",
   "metadata": {},
   "source": [
    "__Use the following code block to confirm that your implementation is correct using gradient checking. If there are problems with your gradient or loss, go back and fix them!:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cd466ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 4.324310\tanalytic: 4.324310\trelative error: 1.262269e-08\n",
      "numerical: -12.628893\tanalytic: -4.519325\trelative error: 4.729103e-01\n",
      "numerical: 1.721391\tanalytic: -1.730062\trelative error: 1.000000e+00\n",
      "numerical: -1.283641\tanalytic: 0.000000\trelative error: 1.000000e+00\n",
      "numerical: -3.584145\tanalytic: 0.000000\trelative error: 1.000000e+00\n",
      "numerical: 4.318839\tanalytic: 0.000000\trelative error: 1.000000e+00\n",
      "numerical: 1.226058\tanalytic: 0.000000\trelative error: 1.000000e+00\n",
      "numerical: -18.543648\tanalytic: -0.206529\trelative error: 9.779704e-01\n",
      "numerical: -1.958505\tanalytic: 0.000000\trelative error: 1.000000e+00\n",
      "numerical: -1.407731\tanalytic: 0.000000\trelative error: 1.000000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8451)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # from torch.autograd import gradcheck\n",
    "from random import randrange\n",
    "\n",
    "def grad_check(f, x, analytic_grad, num_checks=10, h=1e-3):\n",
    "    sum_error = 0\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape]) #randomly sample value to change\n",
    "\n",
    "        oldval = x[ix].item()\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evaluate f(x + h)\n",
    "        x[ix] = oldval - h # increment by h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic) + 1e-8)\n",
    "        sum_error += rel_error\n",
    "        print('numerical: %f\\tanalytic: %f\\trelative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
    "    return sum_error / num_checks\n",
    "\n",
    "# Create some test data:\n",
    "num_classes = 10\n",
    "features_dim = 20\n",
    "num_items = 100\n",
    "Theta = torch.randn((features_dim, num_classes))\n",
    "X = torch.randn((num_items,features_dim))\n",
    "y = torch.torch.randint(0, num_classes, (num_items, 1))\n",
    "\n",
    "# compute the analytic gradient\n",
    "grad = softmax_regression_loss_grad(Theta, X, y)\n",
    "    \n",
    "# run the gradient checker    \n",
    "grad_check(lambda th: softmax_regression_loss(th, X, y), Theta, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae57939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta = torch.Tensor([[1, 0], [0, 1]])\n",
    "X = torch.Tensor([[1, 0], [0, 1]])\n",
    "y = torch.LongTensor([[0], [1]])\n",
    "assert torch.abs(softmax_regression_loss(Theta, X, y) - 0.6265) < 0.0001\n",
    "grad = softmax_regression_loss_grad(Theta, X, y)\n",
    "assert torch.torch.allclose(torch.abs(grad/0.2689), torch.ones_like(grad), atol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45acdb24",
   "metadata": {},
   "source": [
    "## Training Softmax regression with gradient descent on real data\n",
    "\n",
    "We'll now try gradient descent with our softmax regression using the digits dataset. As before, when we looked at logistic regression, we load the data and create test and training sets. Note that this time we'll use all the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b7bc0865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([260, 65])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data, target = load_digits(n_class=10, return_X_y=True)\n",
    "X = torch.Tensor(data)\n",
    "y = torch.Tensor(target) #convert to pytorch Tensors\n",
    "X = torch.cat((X, torch.ones((X.shape[0], 1))), 1) # append a column of 1's to the X's\n",
    "X /= 255\n",
    "y = y.reshape(-1, 1) # reshape y into a column vector\n",
    "y = y.type(torch.LongTensor)\n",
    "\n",
    "# We're also going to break the data into a training set for computing the regression parameters\n",
    "# and a test set to evaluate the predictive ability of those parameters\n",
    "perm = torch.randperm(y.shape[0])\n",
    "X_train = X[perm[0:260], :]\n",
    "y_train = y[perm[0:260]]\n",
    "X_test = X[perm[260:], :]\n",
    "y_test = y[perm[260:]]\n",
    "\n",
    "X_train.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b165a0",
   "metadata": {},
   "source": [
    "We now define a simple gradient descent loop to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a1362a1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (65) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-71e159aadace>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_regression_loss_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta_gd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtheta_gd\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Loss: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoftmax_regression_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta_gd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (65) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "theta_gd = torch.rand((X_train.shape[1], 10))\n",
    "theta_gd.size()\n",
    "for e in range(0, 1000):\n",
    "    gr = softmax_regression_loss_grad(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "    if e%100 == 0:\n",
    "        print(\"Training Loss: \", softmax_regression_loss(theta_gd, X_train, y_train))\n",
    "\n",
    "# Compute the accuracy of the test set\n",
    "proba = torch.softmax(X_test @ theta_gd, 1)\n",
    "print(float((proba.argmax(1)-y_test[:,0]==0).sum()) / float(proba.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ae3bb",
   "metadata": {},
   "source": [
    "# Overparameterisation in softmax regression\n",
    "\n",
    "Softmax regression has an unusual property that it has a \"redundant\" set of parameters. To explain what this means, suppose we take each of our parameter vectors $\\theta^{(j)}$, and subtract some fixed vector $\\psi$ from it, so that every $\\theta^{(j)}$ is now replaced with $\\theta^{(j)}−\\psi$ (for every $j=1,\\dots,k$). Our hypothesis now estimates the class label probabilities as\n",
    "\n",
    "\\begin{align}\n",
    "P(y^{(i)} = k | x^{(i)} ; \\theta)\n",
    "&= \\frac{\\exp((\\theta^{(k)}-\\psi)^\\top x^{(i)})}{\\sum_{j=1}^K \\exp( (\\theta^{(j)}-\\psi)^\\top x^{(i)})}  \\\\\n",
    "&= \\frac{\\exp(\\theta^{(k)\\top} x^{(i)}) \\exp(-\\psi^\\top x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)}) \\exp(-\\psi^\\top x^{(i)})} \\\\\n",
    "&= \\frac{\\exp(\\theta^{(k)\\top} x^{(i)})}{\\sum_{j=1}^K \\exp(\\theta^{(j)\\top} x^{(i)})}.\n",
    "\\end{align}\n",
    "\n",
    "__In other words, subtracting $\\psi$ from every $\\theta^{(j)}$ does not affect our hypothesis’ predictions at all!__ This shows that softmax regression’s parameters are \"redundant\". More formally, we say that our softmax model is \"overparameterised\" meaning that for any hypothesis we might fit to the data, there are multiple parameter settings that give rise to exactly the same hypothesis function $h_\\theta$ mapping from inputs $\\mathbf{x}$ to the predictions.\n",
    "\n",
    "Further, if the cost function $J(\\theta)$ is minimized by some setting of the parameters $(\\theta^{(1)},\\theta^{(2)},\\dots,\\theta^{(k)})$, then it is also minimised by $\\theta^{(1)}-\\psi,\\theta^{(2)}-\\psi,\\dots,\\theta^{(k)}-\\psi)$ for any value of $\\psi$. Thus, the minimiser of $J(\\theta)$ is not unique. \n",
    "\n",
    "(Interestingly, $J(\\theta)$ is still convex, and thus gradient descent will not run into local optima problems. The Hessian is however singular/non-invertible, which causes a straightforward implementation of Newton's method (a second-order optimiser) to run into numerical problems.)\n",
    "\n",
    "Notice also that by setting $\\psi=\\theta^{(K)}$, one can always replace $\\theta^{(K)}$ with $\\theta^{(K)}-\\psi=\\mathbf{0}$ (the vector of all $0$’s), without affecting the hypothesis. Thus, one could \"eliminate\" the vector of parameters $\\theta^{(K)}$ (or any other $\\theta^{(k)}$, for any single value of $k$), without harming the representational power of our hypothesis. Indeed, rather than optimising over the $K \\cdot n$ parameters $(\\theta^{(1)},\\theta^{(2)},\\dots,\\theta^{(k)})$  (where $\\theta^{(k)} \\in \\mathbb{R}^n$, one can instead set $\\theta^{(K)}=\\mathbf{0}$  and optimize only with respect to the $(K-1) \\cdot n$ remaining parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f79c44",
   "metadata": {},
   "source": [
    "__Use the following block to implement the softmax gradients for the case where the final column of the parameters theta is fixed to be zero:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0410f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def softmax_regression_loss_grad_0(Theta, X, y):\n",
    "    '''Implementation of the gradient of the softmax loss function, with the parameters of the\n",
    "    last class fixed to be zero.\n",
    "    \n",
    "    Theta is the matrix of parameters, with the parameters of the k-th class in the k-th column; \n",
    "            K-1 classes are included, and the parameters of the last class are implicitly zero.\n",
    "    X contains the data vectors (one vector per row)\n",
    "    y is a column vector of the targets\n",
    "    '''\n",
    "    \n",
    "    # add the missing column of zeros:\n",
    "    Theta = torch.cat((Theta, torch.zeros(Theta.shape[0],1)), 1)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # remove the last column from the gradients\n",
    "    grad = grad[0:grad.shape[0], 0:grad.shape[1]-1]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "83d9046f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-9767adf20770>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_regression_loss_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgrad0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_regression_loss_grad_0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-111-76a80d9b53b7>\u001b[0m in \u001b[0;36msoftmax_regression_loss_grad_0\u001b[1;34m(Theta, X, y)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# remove the last column from the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Theta = torch.Tensor([[1, 0], [0, 0]])\n",
    "X = torch.Tensor([[1, 0], [0, 1]])\n",
    "y = torch.LongTensor([[0], [1]])\n",
    "\n",
    "grad = softmax_regression_loss_grad(Theta, X, y)\n",
    "grad0 = softmax_regression_loss_grad_0(Theta[:,0:grad.shape[1]-1], X, y)\n",
    "assert torch.torch.allclose(grad[:,0:grad.shape[1]-1], grad0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b0638ddd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-ae7d3b25f833>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_regression_loss_grad_0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta_gd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtheta_gd\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-111-76a80d9b53b7>\u001b[0m in \u001b[0;36msoftmax_regression_loss_grad_0\u001b[1;34m(Theta, X, y)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# remove the last column from the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "theta_gd = torch.rand((X_train.shape[1], 9))\n",
    "\n",
    "for e in range(0, 1000):\n",
    "    gr = softmax_regression_loss_grad_0(theta_gd, X_train, y_train)\n",
    "    theta_gd -= alpha * gr\n",
    "\n",
    "theta_gd = torch.cat((theta_gd, torch.zeros(theta_gd.shape[0], 1)), 1)\n",
    "proba = torch.softmax(X_test @ theta_gd, 1)\n",
    "print(float((proba.argmax(1)-y_test[:,0]==0).sum()) / float(proba.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540bfa27",
   "metadata": {},
   "source": [
    "Finally, we can run gradient descent with our reduced paramter gradient function, and confirm that the results are similar to before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aecf6b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b749c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32c8ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7886, -1.3492,  0.9777],\n",
       "        [-0.0462, -0.2879, -1.0416],\n",
       "        [-0.1155,  0.4664, -0.1692]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0a82a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7886, -1.3492,  0.9777])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426d28f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0305, -0.6596,  0.1758],\n",
       "        [-0.3989,  0.6862, -0.6219],\n",
       "        [ 0.8953,  1.1149, -0.1720]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn(3,3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db5acdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6010, -0.3560,  0.8096],\n",
       "        [-0.7701, -1.3284,  0.3501],\n",
       "        [-0.2185,  0.2077, -0.2813]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b6fa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8127,  0.8900,  0.1719],\n",
       "        [ 0.0184, -0.1976,  0.6478],\n",
       "        [-0.1034,  0.5200,  0.0291]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1faa6ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6010, -0.3560,  0.8096],\n",
       "        [-0.7701, -1.3284,  0.3501],\n",
       "        [-0.2185,  0.2077, -0.2813]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ce5d73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3925, -0.7487, -0.8038])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8e2c5f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1774, -2.2462, -2.4115])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:,0]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a53e6e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1774, -2.2462, -2.4115],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = torch.zeros((100,3))\n",
    "grad[0]=(b [:,0]*3)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47495b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4470, -1.3650, -0.6870])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:,0].t()*a[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40f464e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.4990)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:,0]@a[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e337136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6396, 0.2554, 0.5031])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(b[:,0].t()*a[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19921c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(b[:,0].t()*a[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3191b76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1233, 6.1912, 2.3505],\n",
       "        [0.8020, 1.3653, 5.4525],\n",
       "        [1.4036, 0.3809, 0.6920]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54c28516",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum = torch.sum(torch.exp(a),dim=1 , keepdim = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a446ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum = torch.sum(torch.exp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2ab0fcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.7612)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee2d1923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.6649)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sum[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b69f241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "a == b\n",
    "1* (a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d45d5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a139cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
